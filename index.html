
<!DOCTYPE html><!-- Last Published: Fri Mar 27 2020 21:28:31 GMT+0000 (Coordinated Universal Time) -->
<html data-wf-domain="www.matthewtancik.com" data-wf-page="5e6fb768456f961381500a5f" data-wf-site="51e0d73d83d06baa7a00000f">
<head>
  <meta charset="utf-8"/>
  <title>Saliency-Driven Editing</title>
  <meta content="Learning to Reduce Visual Distraction with Deep Saliency Models" name="description"/>
  <meta content="Saliency-Driven Editing" property="og:title"/>
  <meta content="An end-to-end method for cross-structural motion retargeting" property="og:description"/>
  <meta content="summary" name="twitter:card"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
  <link href="./saliency_driven.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic","Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic","Changa One:400,400italic","Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic","Varela Round:400","Bungee Shade:regular","Roboto:300,regular,500"]  }});</script>
  <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/logo.png" rel="shortcut icon" type="image/x-icon"/>
  <link href="images/logo.png" rel="apple-touch-icon"/>
  <style>
.wf-loading * {
    opacity: 0;
}
</style></head>
<body>
  <div class="section hero nerf-_v2">
    <div class="container-2 nerf_header_v2 w-container">
      <h1 class="nerf_title_v2">Deep Saliency Prior</h1>
      <br></br>
<!--      <h1 class="nerf_subheader_v2"></h1>-->
      <div class="nerf_authors_list_single w-row">
        <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="http://kfiraberman.github.io/" target="_blank" class="nerf_authors_v2">Kfir Aberman<span class="text-span_nerf_star">*</span></a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://research.google/people/106417/" target="_blank" class="nerf_authors_v2">Junfeng He<span class="text-span_nerf_star">*</span></a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://yossigandelsman.github.io/" target="_blank" class="nerf_authors_v2">Yossi Gandelsman</a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://research.google/people/InbarMosseri/" target="_blank" class="nerf_authors_v2">Inbar Mossari</a></div>
          <!-- <div class="w-col w-col-2 w-col-small-4 w-col-tiny-4"><a href="https://www.cs.tau.ac.il/~dcor/" target="_blank" class="nerf_authors_v2">David E. Jacobs<span class="text-span_nerf">5,1</span></a></div> -->
          <!-- <div class="w-col w-col-2 w-col-small-4 w-col-tiny-4"><a href="https://cfcs.pku.edu.cn/baoquan" target="_blank" class="nerf_authors_v2">Kai Kohlhoff<span class="text-span_nerf">2,1</span></a></div> -->
        </div>
        <div class="nerf_authors_list_single w-row">
        <!-- <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="http://kfiraberman.github.io/" target="_blank" class="nerf_authors_v2">Kfir Aberman<span class="text-span_nerf_star">*</span></a></div> -->
          <!-- <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://peizhuoli.github.io/" target="_blank" class="nerf_authors_v2">Junfeng He<span class="text-span_nerf_star">*</span></a></div> -->
          <!-- <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://www.cse.huji.ac.il/~danix/" target="_blank" class="nerf_authors_v2">Yossi Gandelsman</a></div> -->
          <!-- <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://igl.ethz.ch/people/sorkine/" target="_blank" class="nerf_authors_v2">Inbar Mossari</a></div> -->
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="http://graphics.stanford.edu/~dejacobs/" target="_blank" class="nerf_authors_v2">David E. Jacobs<span class="text-span_nerf">5,1</span></a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://research.google/people/KaiKohlhoff/" target="_blank" class="nerf_authors_v2">Kai Kohlhoff<span class="text-span_nerf">2,1</span></a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="https://research.google/people/106214/" target="_blank" class="nerf_authors_v2">Yael Pritch<span class="text-span_nerf">2,1</span></a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-3"><a href="http://people.csail.mit.edu/mrub/" target="_blank" class="nerf_authors_v2">Michael Rubinstein<span class="text-span_nerf">2,1</span></a></div>
        </div>
   <!--      <div class="nerf_authors_list_single w-row">
        <div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="http://kfiraberman.github.io/" target="_blank" class="nerf_authors_v2">Kfir Aberman<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">1,5*</span></a></div>
          <div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://peizhuoli.github.io/" target="_blank" class="nerf_authors_v2">Junfeng He<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">2,1*</span></a></div>
          <div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://www.cse.huji.ac.il/~danix/" target="_blank" class="nerf_authors_v2">Yossi Gandelsman<span class="text-span_nerf">3,1</span><span class="superscript"></span></a></div>
          <div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://igl.ethz.ch/people/sorkine/" target="_blank" class="nerf_authors_v2">Inbar Mossari<span class="text-span_nerf">4,1</span><span class="superscript"></span></a></div>
          <div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://www.cs.tau.ac.il/~dcor/" target="_blank" class="nerf_authors_v2">David E. Jacobs<span class="text-span_nerf">5,1</span></a></div>
          <div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://cfcs.pku.edu.cn/baoquan" target="_blank" class="nerf_authors_v2">Kai Kohlhoff<span class="text-span_nerf">2,1</span></a></div>
        </div> -->
<!--           <div class="columns-6 w-row"><div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4">
            <div class="nerf_mobile_inst"><span class="text-span_nerf">1 </span>Beijing Film Academy</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">2</span>Peking University</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">3</span>Hebrew University</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">4</span>ETH Zurich</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">5</span>Tel-Aviv University</div></div>
          </div>
            <div class="nerf_authors_list_single nerf_authors_affiliation w-row">
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Beijing Film Academy</h1></div>
              <div class="column w-col w-col-2"><h1 class="nerf_affiliation_v2">Peking University</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Hebrew University</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">ETH Zurich</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Tel-Aviv University</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Peking University</h1></div>
            </div> -->
              <div class="div-block-10"><div class="nerf_affiliation_v2">Google Research</div></div>
              <br></br>
              <!-- <img src="./images/teaser.png" alt=""/> -->
                <div>
                  <span class="center"><img src="images/teaser-web.png"></span>
                  <!-- <div class="w-col w-col-4"><a class="examples_header">Input</a></div>
                  <div class="w-col w-col-4"><a class="examples_header">Output 1</a></div>
                  <div class="w-col w-col-4"><a class="examples_header">Output 2</a></div> -->
                </div>
              <div class="link_column_nerf_v2 w-row">
                <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                  <a href="./saliency_driven_editing_preprint.pdf" target="_blank" class="link-block w-inline-block">
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"/></a>
                <!-- <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w" sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.84375px, 56.6875px" class="paper_img image-8_nerf"/></a> -->
              </div>
              <div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="./index.html" target="_blank" class="link-block w-inline-block">
                  <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7136849ee3b0a0c6a95151_database.svg" alt="paper" class="paper_img image-8_nerf nerf_db_icon"/></a>
                </div>
              <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                <a href="./index.html" target="_blank" class="link-block w-inline-block">
                <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"/></a>
              </div>
              </div>
                  <div class="paper_code_nerf w-row">
                    <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                      <div class="text-block-2"><strong class="bold-text-nerf_v2">Paper</strong></div>
                    </div>
                      <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="text-block-2">
                        <strong class="bold-text-nerf_v2">Supplementary Material</strong></div>
                      </div>
                        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                          <div class="text-block-2"><strong class="bold-text-nerf_v2">Code (coming soon)</strong></div>
                        </div>
                      </div>
                          <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div></div>

                          <div data-anchor="slide1" class="section nerf_section">
                            <!-- <div class="w-container"><h2 class="grey-heading_nerf">Overview Video</h2>
                              <div style="padding-top:56.17021276595745%" id="w-node-e5e45b1d55ac-81500a5f" class="w-embed-youtubevideo stega_movie youtube">
                                <iframe src="https://www.youtube.com/embed/ym8Tnmiz5N8?rel=1&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameBorder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                              </div></div> -->
                            </div>
                            <div data-anchor="slide1" class="section nerf_section">
                              <div class="grey_container w-container">
                                <p class="paragraph-3 nerf_text abstract">We show that a saliency model trained to predict human eye-gaze can drive a
range of powerful editing effects for reducing distraction in images, without any
additional supervision. Given an image and a region to edit, we cast the problem of
reducing distraction as an optimization over a composition of a differentiable image
editing operator and a state-of-the-art saliency model. We demonstrate several
operators, including: a recoloring operator, which applies a color transform that
camouflages and blends distractors into their surroundings; a warping operator,
which warps less salient image regions to cover distractors, gradually collapsing
objects into themselves and effectively removing them (an effect akin to inpainting);
a GAN operator, which uses a semantic prior to fully replace image regions
with plausible, less salient alternatives. The resulting effects are consistent with
cognitive research on the human visual system (e.g., since color mismatch is salient,
the recoloring operator learns to harmonize objects’ colors with their surrounding
to reduce their saliency), and, importantly, are all achieved solely through the
guidance of the pretrained saliency model, with no additional training data. We
present results on a variety of natural images and conduct a perceptual study to
evaluate and validate the changes in viewers’ eye-gaze between the original images
and our edited results.</p>

<!-- <img src="./images/architecture-web.png" alt="" class="nerf_network"/>

<p class="paragraph-3 nerf_text">Our main technical contribution is the introduction of novel differentiable convolution, pooling, and unpooling operators, which are skeleton-aware, meaning that they explicitly account for the skeleton's hierarchical structure and joint adjacency.</p>

<img src="./images/conv_and_pool.png" alt="" class="nerf_network"/>

<p class="paragraph-3 nerf_text">Our optimization framework gradually removes thedistracting object by covering it with nearby pixels. Texture mismatch results in high saliency, thus,saliency model guides the warp operator towards a seamless completion of the region.</p>

<img src="./images/warp_sequence.png" alt="" class="nerf_network"/> -->

</div></div>

<!-- <div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">Approach</h2>
  <p class="paragraph-3 nerf_text nerf_results_text">Given an input image, a region of interest mask and an operator, we cast the problem of reducing distraction as an optimization over a composition of a differentiable image editing operator and a state-of-the-art saliency model. Our approach generates an image with high-fidelity to the input image outside of the mask, and with reduced saliency inside it:</p>
  <span class="center">
    <img class="skeleton_example" src="images/architecture-web.png">
  </span>
</div>
</div> -->

<div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">Results</h2>
  <p class="paragraph-3 nerf_text nerf_results_text">Our optimization framework gradually removes thedistracting object by covering it with nearby pixels. Texture mismatch results in high saliency, thus,saliency model guides the warp operator towards a seamless completion of the region:</p>
  <span class="center"><img class="skeleton_example" src="images/results.png"></span>
</div></div>

<!-- <div data-anchor="slide1" class="section nerf_section"></div>
<div class="white_section_nerf">
  <div class="w-container">
    <h2 class="grey-heading_nerf">Saliency Driven Image Warping</h2>
    <p class="paragraph-3 nerf_text nerf_results_text">Our optimization framework gradually removes thedistracting object by covering it with nearby pixels. Texture mismatch results in high saliency, thus,saliency model guides the warp operator towards a seamless completion of the region:</p>
    <span class="center"><img class="skeleton_example" src="images/warp_sequence.png"></span>
</div></div> -->


<div class="white_section_nerf">
  <div class="grey_container w-container">
  <h2 class="grey-heading_nerf">Reducing Distraction in a Video Conference Calls</h2>
  <p class="paragraph-3 nerf_text nerf_results_text"> Our approach + background blur can reduce visual attention drawn to distracting regions, while maintaining the structural integrityof the subject’s environment. Compare with the common background blur effect, which leaves colorful, attention-grabbing blobs in the background:</p>
    <p align="left">&nbsp;</p>
  <table width="600" border="0" align="center">
    <tbody>
      <tr>
        <td align="center"><video width="800" controls="controls" loop="loop">
            <source src="images/ours_vs_bg_blur.mp4" type="video/mp4" />
          </video></td>
      </tr>
    </tbody>
  </table>
  <!-- <span class="center"><img class="skeleton_example" src="images/results.png"></span> -->
</div>
</div>

<div data-anchor="slide1" class="section nerf_section"></div>
<div class="white_section_nerf">
  <div class="w-container">
    <h2 class="grey-heading_nerf">Saliency Increase in StyleGAN space:</h2>
    <p class="paragraph-3 nerf_text nerf_results_text">The output image (right) is achieved by learning directions in the latent space, such that the saliency of the original image (left) is increased in the region of interest (marked in red on the corresponding saliency map). The found directions are semantically meaningful and natural (adding a moustache and adding prominent domes):</p>
    <span class="center"><img class="skeleton_example" src="images/gan_inc.png"></span>
</div></div>

<div data-anchor="slide1" class="section nerf_section"></div>
<div class="white_section_nerf">
  <div class="grey_container w-container">
    <h2 class="grey-heading_nerf">Validation through Real Eye-Gaze Measurement:</h2>
    <p class="paragraph-3 nerf_text nerf_results_text">Samples of real eye-gaze saliency maps measured in our perceptual study Each pair in the first row show an original image (left) with a region of interest on top (red border) and our result (right). The second row depicts the corresponding average eye-gaze maps across participants in the study:</p>
    <span class="center"><img class="skeleton_example" src="images/eye_gaze.png"></span>
</div></div>




<div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">BibTeX</h2>
  <div class="grey_container w-container">
    <div class="bibtex">
          <pre><code>@article{aberman2021learning,
  author = {Aberman, Kfir and He, Junfeng and Gandelsman, Yossi and Mossari, Inbar and Jacobes, David E. and Kohlhoff, Kai and Pritch, Yael and Rubinstein, Michael},
  title = {Learning to Reduce Visual Distraction with Deep Saliency Models},
  publisher = {Arxiv}
}</code></pre>
      </div>
    </div>
    </div>
    </div>




<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.3057c11af.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]--></body></html>